<div align="center">

<table border="0">
<tr>
<td><img src="figures/car_bench_evaluator_pb.png" alt="CAR-bench Evaluator" width="80"></td>
<td><h1>CAR-bench: Agentified CAR-bench Evaluation Framework with AgentBeats</h1></td>
</tr>
</table>

[![Paper](https://img.shields.io/badge/Paper-Coming%20Soon-orange.svg)](#citation)
[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![YouTube](https://img.shields.io/badge/YouTube-Demo-red.svg?logo=youtube)](https://youtu.be/jnS8R59XEWA)
[![AgentBeats](https://img.shields.io/badge/AgentBeats-Green%20Agent-green.svg)](https://agentbeats.dev/johanneskirmayr/car-bench-evaluator)
[![AgentBeats](https://img.shields.io/badge/AgentBeats-Purple%20Agent-purple.svg)](https://agentbeats.dev/johanneskirmayr/car-bench-agent)

*Evaluation framework for CAR-bench using the A2A protocol and AgentBeats platform*

[Overview](#overview) â€¢ [Setup](#setup) â€¢ [Usage](#usage) â€¢ [Evaluation](#evaluation) â€¢ [Architecture](#architecture) â€¢ [Links](#important-links)

</div>

---

## Overview

[**CAR-bench**](https://github.com/CAR-bench/car-bench) is instantiated in an **automotive in-car voice assistant domain** and evaluates the **epistemic reliability** of multi-turn, tool-using LLM agents in realistic, user-facing environments under uncertainty, ambiguity, and capability constraints. Unlike existing agent benchmarks that primarily assess task completion under idealized and fully specified conditions, CAR-bench shifts the evaluation focus toward whether an agent knows **when it can act**, **when it must gather more information**, and **when it should explicitly refuse or defer action** - critical capabilities for deployment in real-world applications.

The automotive in-car voice assistant domain naturally combines incomplete and ambiguous user requests, heterogeneous APIs, mutable environment state, and strict domain policies. CAR-bench features:

- ğŸš— **58 interconnected tools** across navigation, vehicle control, charging, and productivity
- ğŸ“‹ **19 domain-specific policies** that the agent has to follow for task success  
- ğŸ—£ï¸ **LLM-simulated user** for dynamic multi-turn evaluation
- ğŸŒ **Large-scale environment**: 48 cities, 130K POIs, 1.7M routes, 100 calendars/contacts
- ğŸ“ **254 realistic tasks** across three task types spanning intent interpretation, multi-turn planning and action execution, uncertainty handling, and hallucination avoidance

<div align="center">
<img src="figures/car_bench_parts_overview.png" alt="CAR-bench Components" width="80%">
<p><em>CAR-bench components: ğŸŸ© Green-Agent: (a) LLM-simulated user generates multi-turn messages from Task description; (d-f) Mutable environment state, fixed context variables, and static databases; ğŸŸª Purple-Agent: (b) Agent under test guided by domain policies; (c) 58 interconnected tools provided by green agent to interact with environment and user. </em></p>
</div>

### Task Types: Three Complementary Evaluation Dimensions

CAR-bench comprises **254 tasks** across three task types designed to test different aspects of agent reliability:

| Task Type | Train | Test | Description |
|-----------|-------|------|-------------|
| **Base** | 50 | 50 | Agents must correctly interpret intent, plan across turns, invoke tools, and comply with policies to achieve a well-defined goal |
| **Hallucination** | 48 | 50 | Deliberately unsatisfiable tasks (missing tools, unavailable data, unsupported capabilities) testing whether agents acknowledge limitations rather than fabricating responses |
| **Disambiguation** | 31 | 25 | Underspecified or ambiguous requests requiring agents to actively resolve uncertainty through user clarification or internal information gathering before acting |

**Key Testing Dimensions:**
- âœ… **Multi-turn planning**: 1-9 actions per task requiring sequential reasoning
- âœ… **Policy compliance**: Adherence to 19 safety and domain-specific policies
- âœ… **Limit awareness**: Recognizing and refusing unsatisfiable requests
- âœ… **Uncertainty handling**: Resolving ambiguity through clarification or context

### Evaluation: Consistency Metrics for Deployment Readiness

Each task is evaluated using multiple fine-grained metrics, including correctness of actions, policy compliance, and tool-calling errors (see [Evaluation](#evaluation)).
To assess whether agents exhibit reliable behavior consistently across repeated interactions, CAR-bench reports **Pass^k and Pass@k** over multiple trials (k=3 in AgentBeats Leaderboard):

- **Pass^k**: Task solved in **all k runs** â†’ measures **consistency** (deployment readiness)
- **Pass@k**: Task solved in **at least one of k runs** â†’ measures **latent capability**

ğŸ“„ **Paper** (in review): Full benchmark details, task construction methodology, and baseline results  
ğŸ”— **Original CAR-bench** ([github.com/CAR-bench/car-bench](https://github.com/CAR-bench/car-bench)): Task definitions, environment implementation, tools & policies, baseline evaluation, analysis scripts.

---

## What This Repository Adds: AgentBeats Integration

This repository **agentifies CAR-bench** for the AgentBeats platform, enabling **standardized, reproducible agent evaluation** via the A2A protocol:

### âœ¨ Key Innovations

- **ğŸŒ Universal Compatibility**: Agentified CAR-bench evaluator (Green Agent) allows any A2A-compatible agent (Purple Agent) to be evaluated without modifying the benchmark
- **ğŸ—ï¸ Green/Purple Architecture**: Clean separation between evaluator (Green Agent) and agent under test (Purple Agent)
- **ğŸ³ Dockerized Deployment**: Local Python development with dockerized deployment for platform-agnostic evaluation

### Architecture at a Glance

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Green Agent (CAR-bench Evaluator)                      â”‚
â”‚  â€¢ Wraps original CAR-bench environment                  â”‚
â”‚  â€¢ Manages 58 tools, 19 policies, LLM-simulated user    â”‚
â”‚  â€¢ Executes tool calls & returns environment responses   â”‚
â”‚  â€¢ Scores agent performance across 6 metrics per task    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â†•  A2A Protocol
                        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Purple Agent (Your Agent Under Test)                    â”‚
â”‚  â€¢ Receives policy & messages (A2A Text part)            â”‚
â”‚  â€¢ Receives available tools (A2A Data part)              â”‚
â”‚  â€¢ Makes decisions using LLM (Claude/GPT/Gemini)         â”‚
â”‚  â€¢ Returns responses (Text) & tool calls (Data)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Setup

### Prerequisites

- **Python 3.11+**
- **uv package manager**: [Install uv](https://docs.astral.sh/uv/getting-started/installation/)
- **API Keys**: Anthropic (purple agent), Gemini (user simulator in green agent)

### Installation

```bash
# 1. Clone repository
git clone https://github.com/CAR-bench/car-bench-agentbeats.git
cd car-bench-agentbeats
```

```bash
# 2. Install dependencies
uv sync --extra car-bench-agent --extra car-bench-evaluator
```

```bash
# 3. Manually Download CAR-bench data (~200MB: navigation, POIs, calendars, contacts)
./scenarios/car-bench/setup.sh
```

```bash
# 4. Configure API keys
cp .env.example .env
# Edit .env with your keys:
#   ANTHROPIC_API_KEY=sk-ant-...
#   GEMINI_API_KEY=...
#   OPENAI_API_KEY=... (optional)
```

---

## Usage

- **Cost**: A single full run over all 100 Base tasks costs approximately $0.08 for the user simulator and $11 for a GPT-5 agent with thinking.

The agentified CAR-bench provides **four evaluation modes** for different stages of development:

### ğŸ“Š Usage Mode Comparison

| Mode | When to Use | Setup | Agents | Results |
|------|-------------|-------|--------|---------|
| **A. Local Python** | Development, debugging | uv run | Local processes | `output/results.json` |
| **B. Docker (Local Build)** | Verify Dockerfiles | `generate_compose.py` | Built from Dockerfiles | `output/results.json` |
| **C. Docker (GHCR Images)** | Pre-deployment validation | `generate_compose.py` | Pulled from registry | `output/results.json` |
| **D. Leaderboard (GitHub Actions)** | Official submission | Fork + PR | AgentBeats Agents | Public leaderboard |

---

### A. Local Python Development (Recommended for Iteration)

**Fastest way to test code changes.** Agents run as local Python processes.

```bash
# Run evaluation with default settings
uv run agentbeats-run scenarios/scenario.toml --show-logs
```
**What happens:**
- âœ… Starts green agent (CAR-bench evaluator) locally
- âœ… Starts purple agent (agent under test) locally
- Note: If you see Error: Some agent endpoints are already in use, change the ports in the scenario TOML (or stop the process using them).

**To see agent logs** (optional), manually listen to them in separate terminals.

**Configuration**: Edit [`scenarios/scenario.toml`](scenarios/scenario.toml)

---

### B. Docker with Local Builds (Verify Dockerization)

**Test your Docker setup before deployment.** Builds images from local Dockerfiles.

```bash
# 1. Generate docker-compose.yml from scenario
python generate_compose.py --scenario scenarios/scenario-docker-local.toml
```

```bash
# 2. Run evaluation (builds images automatically)
mkdir -p output
docker compose up --abort-on-container-exit
```

**What happens:**
- âœ… Builds `green-agent` from [`src/green_car_bench_agent/Dockerfile.car-bench-evaluator`](src/green_car_bench_agent/Dockerfile.car-bench-evaluator)
- âœ… Builds `purple-agent` from [`src/purple_car_bench_agent/Dockerfile.car-bench-agent`](src/purple_car_bench_agent/Dockerfile.car-bench-agent)
- âœ… Creates Docker network for inter-agent communication
- âœ… Runs full evaluation with logs in terminal
- âœ… Saves results to `output/results.json`

**Configuration**: Edit [`scenarios/scenario-docker-local.toml`](scenarios/scenario-docker-local.toml)

---

### C. Docker with Published Images (Pre-Deployment Validation)

**Test with production images before submitting to leaderboard.** Uses images from GitHub Container Registry.

Agents in this repository are published via the [publish.yml](.github/workflows/publish.yml) GitHub Actions workflow.
Alternatively, build and push your own images manually:
```bash
docker build --platform linux/amd64 \
    -f src/purple_car_bench_agent/Dockerfile.car-bench-agent \
    -t ghcr.io/yourusername/your-agent:latest .
# Always build linux/amd64 images for GitHub Actions compatibility
docker push ghcr.io/yourusername/your-agent:latest
```

```bash
# Update scenario-ghcr.toml with your image URLs
python generate_compose.py --scenario scenarios/scenario-ghcr.toml
mkdir -p output
docker compose up --abort-on-container-exit
```

**Configuration**: Edit [`scenarios/scenario-ghcr.toml`](scenarios/scenario-ghcr.toml) with your GHCR image URLs

---

### D. Official Leaderboard Submission (GitHub Actions)

**For reproducible, public evaluation results.**

This mode is **not in this repository**â€”it uses the official leaderboard infrastructure:

1. **Register agents** on [agentbeats.dev](https://agentbeats.dev) to get agent ID
2. **Fork** the leaderboard repository: [github.com/CAR-bench/car-bench-leaderboard-agentbeats](https://github.com/CAR-bench/car-bench-leaderboard-agentbeats)
3. **Configure GitHub Secrets** with your API keys
4. **Edit `scenario.toml`** in your fork with your agent ID
5. GitHub Actions runs evaluation â†’ **Submit pull request** â†’ Results published to leaderboard when maintainers merge PR

---

## Scenario Configuration

All evaluation settings are controlled via `.toml` files. The `[config]` section maps to CAR-bench parameters:

### Configuration Options

```toml
[config]
# Evaluation parameters
num_trials = 3              # Runs per task (for Pass^k/Pass@k)
task_split = "test"         # "train" or "test"
max_steps = 50              # Max conversation turns per task

# Task selection (per task type)
tasks_base_num_tasks = 2                    # First N tasks (-1 = all)
tasks_hallucination_num_tasks = 0
tasks_disambiguation_num_tasks = 0

# Alternative: Filter by specific task IDs
# tasks_base_task_id_filter = ["base_0", "base_5", "base_10"]
# tasks_hallucination_task_id_filter = ["hallucination_0"]
# tasks_disambiguation_task_id_filter = ["disambiguation_0"]
```

The **green agent** transforms `[config]` into CAR-bench expected arguments.

### Agent Configuration

**Purple agent** (agent under test):
```toml
[[participants]]
name = "agent"
env = { 
    ANTHROPIC_API_KEY = "${ANTHROPIC_API_KEY}",  # From .env file or GitHub Secrets
    AGENT_LLM = "anthropic/claude-haiku-4-5-20251001"  # Model selection
}
```

- **Note**: This can differ based on your purple agent implementation.

**Supported models** for base purple agent: Any LiteLLM-compatible model (Claude, GPT, Gemini, etc.)

**Green agent** (evaluator):
```toml
[green_agent]
env = { 
    GEMINI_API_KEY = "${GEMINI_API_KEY}",  # User simulator model
    CAR_BENCH_DATA_DIR = "/path/to/data"   # Data directory
}
```

- **Note**: The env line in the .toml need to be one-liners.

---

## Evaluation

### Metrics: Multi-Dimensional Scoring

Each task is evaluated across up to **6 automated metrics** corresponding to its task type:

#### Base Tasks (100 tasks)
- `r_actions_final` (0/1): Did agent reach the correct final environment state through its actions? - Code-Based.
- `r_actions_intermediate` (0/1): Were intermediate state changes correct (order-insensitive)? - Code-Based.
- `r_tool_subset` (0/1): Did agent use all required information-gathering tools? - Code-Based.
- `r_tool_execution_errors` (0/1): Were tool calls syntactically valid? - Code-Based.
- `r_policy_errors` (0/1): Did agent comply with all 19 policies? - 12 Code-Based, 7 LLM-as-a-Judge-Based.
- `r_user_end_conversation` (0/1): Always 1.0 for base tasks. - LLM-as-a-Judge-Based.

**Task reward**: 1 if all metrics are 1, else 0

#### Hallucination Tasks (98 tasks)
- `r_tool_execution_errors` (0/1)
- `r_policy_errors` (0/1)
- `r_user_end_conversation` (0/1): **Critical**â€”1.0 if agent acknowledges inability, 0.0 if hallucinates. - LLM-as-a-Judge-Based (with clear instructions/context).

**Task reward**: 1 if all metrics are 1, else 0

#### Disambiguation Tasks (56 tasks)
- All base metrics **+**
- `r_user_end_conversation` (0.0-1.0): **Critical**â€”0.0 if agent acts without clarifying OR asks when unnecessary - LLM-as-a-Judge-Based (with clear instructions/context).

**Task reward**: 1 if all metrics are 1, else 0

### Consistency Metrics: Pass^k vs Pass@k

Given `k` trials per task:

- **Pass^k**: Task passes **all k trials** â†’ measures **consistency** and deployment readiness
- **Pass@k**: Task passes **at least 1 of k trials** â†’ measures **latent capability**

**Example with k=3:**
```
Task base_0: âœ“ âœ— âœ—   â†’ Pass^3 = 0, Pass@3 = 1 (inconsistent)
Task base_1: âœ“ âœ“ âœ“   â†’ Pass^3 = 1, Pass@3 = 1 (reliable!)
```

**Aggregate scores**: Average Pass^k / Pass@k across all tasks



### Project Structure

```
src/
â”œâ”€â”€ agentbeats/                    # AgentBeats framework
â”‚   â”œâ”€â”€ green_executor.py          # Base class for green agents
â”‚   â””â”€â”€ run_scenario.py            # Local evaluation runner
â”œâ”€â”€ green_car_bench_agent/         # CAR-bench evaluator (green agent)
â”‚   â”œâ”€â”€ car_bench_evaluator.py     # Main evaluator wrapping CAR-bench
â”‚   â”œâ”€â”€ server.py                  # A2A server entrypoint
â”‚   â””â”€â”€ Dockerfile.car-bench-evaluator
â””â”€â”€ purple_car_bench_agent/        # Template agent (purple agent)
    â”œâ”€â”€ car_bench_agent.py         # Agent implementation
    â”œâ”€â”€ server.py                  # A2A server entrypoint
    â””â”€â”€ Dockerfile.car-bench-agent

scenarios/
â”œâ”€â”€ scenario.toml                  # Local Python mode config
â”œâ”€â”€ scenario-docker-local.toml     # Local Docker build config
â””â”€â”€ scenario-ghcr.toml             # Published images config

scenarios/car-bench/car-bench/     # Original CAR-bench (git submodule - cloned via 'uv sync')
â””â”€â”€ car_bench/                     # Environment, tools, user simulator, mock data (130K POIs, 1.7M routes, etc.)
    â”œâ”€â”€ envs/                      # Environment, tools, user simulator
```

---

## Building Custom Agents

Want to build and test your own agent? **Replace the purple agent** while keeping the green agent unchanged:

### Steps

1. **Implement A2A server** following the purple agent interface ([`src/purple_car_bench_agent/car_bench_agent.py`](src/purple_car_bench_agent/car_bench_agent.py))
2. **Update scenario file** to point to your agent

---

## Citation

**Paper (in review)**: Full benchmark methodology, task construction details, and baseline results will be available soon.

```bibtex
@article{}
```

---

## Important Links

- ğŸ”— **Original CAR-bench**: [github.com/CAR-bench/car-bench](https://github.com/CAR-bench/car-bench)
- ğŸ† **Leaderboard**: [github.com/CAR-bench/car-bench-leaderboard-agentbeats](https://github.com/CAR-bench/car-bench-leaderboard-agentbeats)
- ğŸŸ© **Green Agent (CAR-bench Evaluator)**: [agentbeats.dev/johanneskirmayr/car-bench-evaluator](https://agentbeats.dev/johanneskirmayr/car-bench-evaluator)
- ğŸŸª **Purple Agent (Template Agent)**: [agentbeats.dev/johanneskirmayr/car-bench-agent](https://agentbeats.dev/johanneskirmayr/car-bench-agent)
- ğŸ¥ **YouTube Demo**: [youtu.be/jnS8R59XEWA](https://youtu.be/jnS8R59XEWA)
- ğŸŒ **AgentBeats Platform**: [agentbeats.dev](https://agentbeats.dev)
- ğŸ“– **A2A Protocol**: [a2a-protocol.org](https://a2a-protocol.org)

---

## Contributing & Support

**Questions?** Open an issue or discussion on GitHub

**Contributing:**
- ğŸ› Report bugs via GitHub Issues
- ğŸ¯ Submit improved purple agent implementations
- ğŸ“Š Share evaluation results and insights
- ğŸ”§ Propose new features or evaluation modes

**License**: See [`LICENSE`](LICENSE)

---

<div align="center">

**Built with [AgentBeats](https://agentbeats.dev) â€¢ Evaluating the future of AI agents**

</div>
